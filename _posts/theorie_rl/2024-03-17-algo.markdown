---
layout: post
title:  "Exemple d'algorithmes : Q learning Deep Q Learning"
date:   2024-03-17 18:34:50 +0100
categories: deep_rl
---
<link rel="stylesheet" href="https://picorba.github.io/Rapport-veille-technologique/assets/css/theme_dark.css">


<div class="texte">
J'ai décidé ici de ne présenter que Q table et deep Q table afin de d'illustrer la différence entre RL et DRL. Voici un schéma qui illustre les deux algorithmes :
<img src="https://picorba.github.io/Rapport-veille-technologique/assets/images/table.png" alt="Q Table/Deep Q Table"><br>
Source : https://www.v7labs.com/blog/deep-reinforcement-learning-guide <br>
</div>

# Q Learning

<div class="texte">
L'algorithme Q-Learning repose sur une fonction d'action-valeur appelée fonction Q, qui attribue une valeur à chaque paire (état, action). Cette valeur représente la "qualité" ou la "valeur" de prendre une certaine action dans un certain état. <br>

L'algorithme fonctionne de manière itérative, interagissant avec l'environnement à chaque étape. À chaque étape, l'agent choisit une action à partir de l'état actuel, généralement en utilisant une politique d'exploration/exploitation. Une fois l'action choisie, l'agent l'exécute et observe la récompense résultante ainsi que l'état suivant de l'environnement.
<br><br>
La mise à jour de la fonction Q se fait en utilisant l'équation de mise à jour Q-Learning. Cette équation prend en compte la récompense reçue, la valeur Q de l'état suivant, un taux d'apprentissage, et un facteur de remise. Le but ici n'est pas de rentrer dans les détails mathématiques.
<br><br>
Q-Learning converge vers la politique optimale garantie dans des conditions spécifiques, notamment lorsque l'agent explore suffisamment l'espace d'états et d'actions. Des variantes de l'algorithme, telles que le Deep Q-Network (DQN), ont été développées pour traiter des problèmes spécifiques et pour être appliquées à des environnements plus complexes.
<br><br>
 </div>

# Deep Q Learning 

<div class="texte">

La méthode Deep Q-Learning (DQN) est une technique d'apprentissage par renforcement qui combine l'apprentissage profond (deep learning) et l'algorithme Q-Learning. Dans cette approche, un réseau de neurones profond est utilisé pour apprendre la fonction de valeur d'action (action-value function), qui estime la récompense attendue pour une action donnée dans un état donné.
<br> L'agent apprend en interagissant avec l'environnement et en mettant à jour les poids du réseau de neurones en utilisant la méthode de descente de gradient pour minimiser la différence entre la récompense prédite et la récompense réelle. Le DQN utilise également une technique appelée "replay memory" pour stocker et réutiliser les expériences passées de l'agent, ce qui améliore la stabilité et l'efficacité de l'apprentissage. 

</div>

[Exemples d'applications du Deep Reinforcement Learning](https://picorba.github.io/Rapport-veille-technologique/deep_rl/2024/03/17/exemple_rl.html)
